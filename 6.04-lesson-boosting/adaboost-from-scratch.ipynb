{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Coding AdaBoost From Scratch\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** This code-along is intended to be completed with the instructor.\n",
    "\n",
    "Below we will be coding the AdaBoost algorithm from scratch, then visualizing the results as weak learners are chained onto the meta-estimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Algorithm, Part 1\n",
    "\n",
    "---\n",
    "\n",
    "Recall:\n",
    "\n",
    "### $$ AdaBoost(X) = sign\\left(\\sum_{t=1}^T\\alpha_t h_t(X)\\right) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$.\n",
    "\n",
    "$T$ is the set of weak learners.\n",
    "\n",
    "$\\alpha_t$ is the contribution weight for weak learner $t$.\n",
    "\n",
    "$h_t(X)$ is the prediction of weak learner $t$.\n",
    "\n",
    "$y$ is binary **with values of negative one and one**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Write the AdaBoost estimator function based on the algorithm described above.\n",
    "\n",
    "Your function will need arguments:\n",
    "\n",
    "- `X`: The design matrix.\n",
    "- `estimators`: The weak learners.\n",
    "- `alphas`: The contribution weight of each weak learner.\n",
    "\n",
    "We'll be using this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Algorithm, Part 2\n",
    "\n",
    "---\n",
    "\n",
    "The weak learner classifiers are trained sequentially. After each fit, the importance weights on each observation need to be updated. AdaBoost, like all boosting ensemble methods, focuses the next model's fit on the misclassifications/weaknesses of the prior models.\n",
    "\n",
    "All training examples start with equal importance weighting. When we finish training a classifier, we update the importance weighting of the classifier, represented by alpha $\\alpha$.\n",
    "\n",
    "### $$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right) \\text{where } \\epsilon_t < 1$$\n",
    "\n",
    "Where $\\epsilon_t$ is the misclassification rate for the current classifier:\n",
    "\n",
    "### $$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Write a function to calculate the alpha weight.\n",
    "\n",
    "The function will take arguments:\n",
    "\n",
    "- `y`: Yhe vector of true target values.\n",
    "- `y_hat`: The vector of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Algorithm, Part 3\n",
    "\n",
    "---\n",
    "\n",
    "AdaBoost sets up a weight vector on the observations, denoted as $D_t$ where $t$ is the current model iteration. $D_t$ is a probability distribution that determines how likely it is that a given observation will be selected as part of the training set for the current estimator.\n",
    "\n",
    "The $\\alpha$ weighting of the last fit estimator is used in the equation for the weighting distribution. The updated equation is:\n",
    "\n",
    "### $$ D_{t+1}(i) = D_t(i) e^{-\\alpha_t y_i h_t(x_i)} $$\n",
    "\n",
    "Where $i$ is the vector of observation indices and $x_i$ is the observation at the index, $y_i$ is the target.\n",
    "\n",
    "$h_t$ is the previous model fit in the boosting chain.\n",
    "\n",
    "Then, divide the weights by the sum of weights to normalize them, ensuring that they sum to one and form a probability distribution:\n",
    "\n",
    "### $$ D_{t+1}(i) = \\frac{D_{t+1}(i)}{\\sum_{i=1}^N D_{t+1}(i)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Write a function to update the importance weights.\n",
    "\n",
    "The function will take arguments:\n",
    "\n",
    "- `X`: The design matrix.\n",
    "- `y`: The vector of target values.\n",
    "- `current_D`: The current weights on observations.\n",
    "- `alpha`: The weight on the current estimator.\n",
    "- `estimator`: The current weak learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Write a function that will sample observations according to weightings.\n",
    "\n",
    "The function will take arguments:\n",
    "\n",
    "- `X`: The design matrix.\n",
    "- `y`: The vector of target values.\n",
    "- `D`: The probability weights on observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Create some fake data (provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moons = make_moons(n_samples=30, noise=0.75)\n",
    "X, y = moons\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0\n",
    "y_min, y_max = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Write a function to plot the current AdaBoost iteration (provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_adaboost(X, y, xx, xy, clf, D):\n",
    "    from matplotlib.colors import ListedColormap\n",
    "\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "    # Put the result in a color plot.\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(9,7))\n",
    "    \n",
    "    ss = [30 + 2500*w for w in D]\n",
    "\n",
    "    cm_dark = ListedColormap(['#1F77B4', '#FF7F0E'])\n",
    "    cm_light = ListedColormap(['#729ECE', '#FF9E4A'])\n",
    "    # Plot the training points.\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_dark, s=ss)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, cmap=cm_light, alpha=.25)\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Iterate the AdaBoost algorithm through 10 iterations, with each iteration adding a new weak learner.\n",
    "\n",
    "This will need to follow the procedure outlined in the algorithm formulas above.\n",
    "\n",
    "You will need to keep track of:\n",
    "\n",
    "- `D`: The probability weights for sampling observations.\n",
    "- A list of estimators (weak learners).\n",
    "- A list of alpha weightings on each weak learner.\n",
    "\n",
    "At each iteration, plot AdaBoost with the function provided above and calculate the accuracy of the entire ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
