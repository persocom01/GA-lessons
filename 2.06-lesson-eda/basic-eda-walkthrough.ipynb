{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Walkthrough of Standard EDA Procedure\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Minor updates by David Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "This lesson uses a Boston housing market dataset to walk through a basic exploratory data analysis procedure, starting from the very beginning with loading the data. \n",
    "\n",
    "Though in many if not most cases the EDA procedure will be considerably more involved, this should give you an idea of the basic workflow a data scientist would go through when taking a look at a new dataset.\n",
    "\n",
    "Note: this lesson is strictly exploratory. We will not be formulating any hypotheses about the data or testing them. In many cases you may have formulated a hypothesis before even looking at your data, which could considerably affect your focus and choices in what to investigate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Quickly describe a dataset, including data types, missing values and basic descriptive statistics\n",
    "- Rename columns (series) in a DataFrame\n",
    "- Visualize data distributions with box plots\n",
    "- Standardize variables and explain why this is often a useful thing to do\n",
    "- Calculate and visualize correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "\n",
    "- [Description of the Boston Housing Data](#data_description)\n",
    "- [Loading the data](#load_data)\n",
    "- [Drop unwanted columns](#drop)\n",
    "- [Clean corrupted data](#clean)\n",
    "- [Count null values and drop rows](#drop_nulls)\n",
    "- [Rename columns](#rename)\n",
    "- [Investigate potential outliers with boxplots](#boxplots)\n",
    "- [Look at the correlation between variables](#cov_cor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_description'></a>\n",
    "\n",
    "### Description of the Boston Housing Data columns\n",
    "\n",
    "---\n",
    "\n",
    "The columns of the dataset are coded. The corresponding descriptions are:\n",
    "\n",
    "    CRIM: per capita crime rate by town \n",
    "    ZN: proportion of residential land zoned for lots over 25,000 sq. ft. \n",
    "    INDUS: proportion of non-retail business acres per town \n",
    "    CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "    NOX: nitric oxides concentration (parts per 10 million) \n",
    "    RM: average number of rooms per dwelling \n",
    "    AGE: proportion of owner-occupied units built prior to 1940 \n",
    "    DIS: weighted distances to five Boston employment centers \n",
    "    RAD: index of accessibility to radial highways \n",
    "    TAX: full-value property-tax rate per 10000 dollars\n",
    "    PTRATIO: pupil-teacher ratio by town \n",
    "    LSTAT: % lower status of the population \n",
    "    MEDV: Median value of owner-occupied homes in 1000's of dollars\n",
    "    \n",
    "Each row in the dataset represents a different suburb of Boston.\n",
    "\n",
    "These descriptions of shortened or coded variables are often called \"codebooks\" or data dictionaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_data'></a>\n",
    "\n",
    "### 1. Load the data\n",
    "\n",
    "---\n",
    "\n",
    "Import the csv into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_file = './datasets/housing.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='header'></a>\n",
    "\n",
    "### 2. Describe the basic format of the data and the columns\n",
    "\n",
    "---\n",
    "\n",
    "Use the `.head()` function (and optionally pass in an integer for the number of rows you want to see) to examine what the loaded data looks like. This is a good initial step to get a feel for what is in the csv and what problems may be present.\n",
    "\n",
    "The `.dtypes` attribute tells you the data type for each of your columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first 8 rows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the dtypes of the columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='drop'></a>\n",
    "\n",
    "### 2. Drop unwanted columns\n",
    "\n",
    "---\n",
    "\n",
    "It looks like `Unnamed: 0` is an index. This is redundant, since `pandas` automatically creates an index for us (the bold numbers to the left of the DataFrame).\n",
    "\n",
    "The `.drop()` method can be used to get rid of a column like so:\n",
    "\n",
    "```python\n",
    "df.drop(columns=['list', 'columns', 'to', 'drop'], inplace=True)\n",
    "```\n",
    "\n",
    "The `inplace=True` parameter makes our change permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the index object and the first 20 items in the DataFrame's index \n",
    "# to see that we have these row numbers already:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the unneccesary column:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "\n",
    "### 3. Clean corrupted columns\n",
    "\n",
    "---\n",
    "\n",
    "You may have noticed when we examined the `.dtypes` attribute that two of the columns were of type \"object\", indicating that they are strings. However, we know from the data description above (and we can infer from `df.head()`) that `DIS` and `RAD` should be numeric.\n",
    "\n",
    "It is pretty common to have numeric columns represented as strings in your data if some of the observations are corrupted. It is important to always check the data types of your columns.\n",
    "\n",
    "**What is causing the `DIS` column to be encoded as a string?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the `.map()` method to remove the commas from each cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run `df.dtypes`, you'll notice that `DIS` is still a string. Let's convert it to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.B What is causing the `RAD` column to be a string?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, replace all \"?\" cells with `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='drop_nulls'></a>\n",
    "\n",
    "### 4. Determine how many observations are missing\n",
    "\n",
    "---\n",
    "\n",
    "Having replaced the question marks with `np.nan` values, we know that there are some missing observations for the `RAD` column. \n",
    "\n",
    "When we start to build models with data, null values in observations are (almost) never allowed. It is important to always see how many observations are missing for each column.\n",
    "\n",
    "We can count the null values for each column like so:\n",
    "\n",
    "```python\n",
    "df.isnull().sum()\n",
    "```\n",
    "\n",
    "The `.isull()` method will convert the columns to `True` and `False` values.\n",
    "\n",
    "The `.sum()` method will then sum these boolean columns, and the total number of null values per column will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the null values.** \n",
    "\n",
    "In this case, lets keep it simple and just drop the rows from the dataset that contain null values. If a column has a ton of null values it often makes more sense to drop the column entirely instead of the rows with null values.\n",
    "\n",
    "The `.dropna()` function will drop any rows that have _**ANY**_ null values for you.  Use this carefully as you could drop many more rows than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rename'></a>\n",
    "\n",
    "### 5. Make the column names more descriptive\n",
    "---\n",
    "\n",
    "One minor annoyance is that our column names are not at all intuitive. Here's the codebook again for reference:\n",
    "\n",
    "    CRIM: per capita crime rate by town \n",
    "    ZN: proportion of residential land zoned for lots over 25,000 sq. ft.\n",
    "    INDUS: proportion of non-retail business acres per town \n",
    "    CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "    NOX: nitric oxides concentration (parts per 10 million) \n",
    "    RM: average number of rooms per dwelling \n",
    "    AGE: proportion of owner-occupied units built prior to 1940 \n",
    "    DIS: weighted distances to five Boston employment centers\n",
    "    RAD: index of accessibility to radial highways \n",
    "    TAX: full-value property-tax rate per 10000 dollars\n",
    "    PTRATIO: pupil-teacher ratio by town \n",
    "    LSTAT: % lower status of the population \n",
    "    MEDV: Median value of owner-occupied homes in 1000's of dollars\n",
    "\n",
    "Let's rename these to make them more descriptive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There two popular methods to rename dataframe columns.\n",
    "1. Using a _dictionary substitution_, which is very useful if you only want to rename a few of the columns. This method uses the `.rename()` function.\n",
    "2. Using a _list replacement_, which is quicker than writing out a dictionary, but requires a full list of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary Method\n",
    "new_columns_dict = {\n",
    "        'CRIM':'rate_of_crime',\n",
    "        'ZN':'residential_zone_pct',\n",
    "        'INDUS':'business_zone_pct',\n",
    "        'CHAS':'borders_river',\n",
    "        'NOX':'oxide_concentration',\n",
    "        'RM':'average_rooms',\n",
    "        'AGE':'owner_occup_pct',\n",
    "        'DIS':'dist_to_work',\n",
    "        'RAD':'access_to_highway',\n",
    "        'TAX':'property_tax',\n",
    "        'PTRATIO':'student_teacher_ratio',\n",
    "        'LSTAT':'pct_underclass',\n",
    "        'MEDV':'home_median_value'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Replacement Method\n",
    "new_columns_list = ['rate_of_crime', 'residential_zone_pct', 'business_zone_pct',\n",
    "       'borders_river', 'oxide_concentration', 'average_rooms',\n",
    "       'owner_occup_pct', 'dist_to_work', 'access_to_highway', 'property_tax',\n",
    "       'student_teacher_ratio', 'pct_underclass',\n",
    "       'home_median_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='describe'></a>\n",
    "\n",
    "### 6. Describe the summary statistics for the columns\n",
    "\n",
    "<img src=\"https://snag.gy/07JFa5.jpg\" width=\"350\">\n",
    "\n",
    "---\n",
    "\n",
    "The `.describe()` function gives summary statistics for each of your columns. What are some, if any, oddities you notice about the columns based on this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boxplots'></a>\n",
    "\n",
    "### 7. Plot variables with potential outliers using boxplots.\n",
    "\n",
    "---\n",
    "\n",
    "Here we will use the seaborn package to plot boxplots of the variables we have identified as potentially having outliers.\n",
    "\n",
    "Some notes on seaborn's boxplot keyword argument options:\n",
    "\n",
    "    orient: can be 'v' or 'h' for vertical and horizontal, respectively\n",
    "    fliersize: the size of the outlier points (pixels I think)\n",
    "    linewidth: the width of line outlining the boxplot\n",
    "    notch: show the confidence interval for the median (calculated by seaborn/plt.boxplot)\n",
    "    saturation: saturate the colors to an extent\n",
    "\n",
    "There are more keyword arguments available but those are most relevant for now.\n",
    "\n",
    "_If you want to check out more, place your cursor in the `boxplot` argument bracket and press `shift+tab` (Press four times repeatedly to bring up detailed documentation)._\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate of crime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent owner occupied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# business zone percent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='cov_cor'></a>\n",
    "\n",
    "### 8. Correlation matrices\n",
    "\n",
    "---\n",
    "\n",
    "A great way to easily get a feel for linear relationships between your variables is with a correlation matrix.\n",
    "\n",
    "Below is the formula for the correlation between two variables $X$ and $Y$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.A Correlation\n",
    "\n",
    "### $$ \\text{pearson correlation}\\;r = cor(X, Y) =\\frac{cov(X, Y)}{std(X)std(Y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.B The correlation matrix\n",
    "\n",
    "We can see the correlation between all the numeric variables in our dataset by using the `.corr()` method.\n",
    "\n",
    "It's useful to get a feel for which columns are correlated. The `.corr() method` can help you decide what is worth investigating further (though with a lot of variables, the matrix can be a bit overwhelming...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be difficult to spot any outliers simply by staring at our correlation matrix. To help get around this issue, let's use Seaborn's `.heatmap()` method along with our correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
