{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Logistic Regression\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Matt Brems (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Distinguish between regression and classification problems.\n",
    "- Understand how logistic regression is similar to and different from linear regression.\n",
    "- Understand the math behind the logit link function (and logistic function).\n",
    "- Plot the logistic regression for predicting admittance from GPA.\n",
    "- Understand how to interpret the coefficients of logistic regression.\n",
    "- Know the benefits of logistic regression as a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "---\n",
    "\n",
    "Logistic regression is a natural bridge to connect regression and classification.\n",
    "- Logistic regression is the most common binary classification algorithm.\n",
    "- Because it is a regression model, logistic regression will predict continuous values.\n",
    "    - Logistic regression will predict continuous probabilities between 0 and 1.\n",
    "    - Example: What is the probability that someone shows up to vote?\n",
    "- However, logistic regression almost always operates as a classification model.\n",
    "    - Logistic regression will use these continuous predictions to classify something as 0 or 1.\n",
    "    - Example: Based on the predicted probability, do we predict that someone votes?\n",
    "\n",
    "In this lecture, we'll only be reviewing the binary outcome case with two classes, but logistic regression can be generalized to multiple classes.\n",
    "\n",
    "**Some examples of when logistic regression could be used:**\n",
    "- Will a user will purchase a product, given characteristics like income, age, and number of family members?\n",
    "- Does this patient have a specific disease based on their symptoms?\n",
    "- Will a person default on their loan?\n",
    "- Is the iris flower in front of me an \"_Iris versicolor_?\"\n",
    "- Given one's GPA and the prestige of a college, will a student be admitted to a specific graduate program?\n",
    "\n",
    "And many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Import LogisticRegression and LinearRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load-data'></a>\n",
    "\n",
    "## Graduate School Admissions\n",
    "\n",
    "---\n",
    "\n",
    "Today, we'll be applying logistic regression to solve the following problem: \"Given one's GPA and the prestige of a college, will a student be admitted to a specific graduate program?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data.\n",
    "admissions = pd.read_csv('../datasets/admissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first five rows.\n",
    "admissions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four columns are:\n",
    "- `admit`: A binary 0/1 variable indicating whether or not a student was admitted, where 1 means admitted and 0 means not admitted.\n",
    "- `gre`: The student's [GRE (Graduate Record Exam)](https://en.wikipedia.org/wiki/Graduate_Record_Examinations) score.\n",
    "- `gpa`: The student's GPA.\n",
    "- `prestige`: A 1-4 rating for the college's \"prestige.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop every row that has an NA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What assumption are we making when we drop rows that have at least one NA in it?</summary>\n",
    "    \n",
    "- We assume that what we drop looks like what we have observed. That is, there's nothing special about the rows we happened to drop.\n",
    "- We might say that what we dropped is a random sample of our whole data.\n",
    "- It's not important to know this now, but the formal term is that our data is missing completely at random.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Suppose I want to check if a value of 1 for prestige means \"most prestigious\" or \"least prestigious.\" How might I find this out?</summary>\n",
    "    \n",
    "- Check a data dictionary to see if the answer exists.\n",
    "- If a data dictionary doesn't exist, compare prestige to admit. You could use [groupby](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.groupby.html) to make a table or you could use a plot.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare prestige to admit using groupby.\n",
    "# Find the mean prestige score of people who were and weren't admitted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificially increase the signal in the data.\n",
    "\n",
    "The signal for detecting admission in the college data is fairly weak — too weak for the purposes of our demonstration. (There are lots of factors to predicting admission other than GPA, test score, and prestige.)\n",
    "\n",
    "**Obviously you would never do this to your real data**... but in order to make the relationship between our predictors and whether or not someone is admitted a lot clearer, we are making 10 copies of the data set and adding an artificial signal for detecting `admit` to the `gpa` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all rows where prestige = 1.\n",
    "admit = \n",
    "\n",
    "# Concatenate ten copies of admit.\n",
    "admit = pd.concat([admit] * 10, axis=0)\n",
    "\n",
    "# Set a random seed.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Add random noise to inflate GPA for people who were admitted.\n",
    "# Subtract random noise to deflate GPA for people who were not admitted.\n",
    "# Again: DO NOT DO THIS IN GENERAL.\n",
    "admit.loc[admit['admit'] == 1, 'gpa'] += np.random.random(size=admit[admit.admit == 1].shape[0])\n",
    "admit.loc[admit['admit'] == 0, 'gpa'] -= np.random.random(size=admit[admit.admit == 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of Notation\n",
    "\n",
    "You're quite familiar with **linear** regression:\n",
    "\n",
    "### $$\n",
    "\\begin{eqnarray*}\n",
    "\\hat{Y} &=& \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 + \\cdots + \\hat{\\beta}_pX_p \\\\\n",
    "&=& \\hat{\\beta}_0 + \\sum_{j=1}^p\\hat{\\beta}_jX_j\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{Y}$ is the predicted value of $Y$ based on all of the inputs $X_j$.\n",
    "- $X_1$, $X_2$, $\\ldots$, $X_p$ are the predictors.\n",
    "- $\\hat{\\beta}_0$ is the estimated intercept.\n",
    "- $\\hat{\\beta}_j$ is the estimated coefficient for the predictor $X_j$, the $j$th column in variable matrix $\\mathbf{X}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pred-admit'></a>\n",
    "\n",
    "## What If We Predicted `admit` With `gpa` Using Linear Regression?\n",
    "\n",
    "---\n",
    "\n",
    "Let's try predicting the `admit` binary indicator using just `gpa` with a linear regression to see what goes wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y and X.\n",
    "y = admit['admit']\n",
    "X = admit[['gpa']] # Because X is usually a matrix, we'll get a \n",
    "                   # reshape request if we don't use double brackets.\n",
    "\n",
    "\n",
    "# Import train_test_split.\n",
    "\n",
    "\n",
    "# Create training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Instantiate a linear regression model.\n",
    "linmod = \n",
    "\n",
    "# Fit our linear regression model to the training data.\n",
    "\n",
    "\n",
    "# Print out intercept and coefficients.\n",
    "print(f'Intercept: {linmod.intercept_}')\n",
    "print(f'Coefficient: {linmod.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot-reg'></a>\n",
    "### What do those coefficients mean?  Plot `admit` (our $Y$) against `gpa` (our $X$).\n",
    "\n",
    "Looking at the plot below, what are problems with using a regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figures.\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# Plot the regression line where gpa predicts admit.\n",
    "x_vals = np.linspace(1, 5, 300)\n",
    "\n",
    "# Plot line of best fit\n",
    "ax.plot(x_vals,\n",
    "        linmod.predict(x_vals[:, np.newaxis]),\n",
    "        color='black', alpha=0.7, lw=4)\n",
    "\n",
    "# Create scatterplot with orange dots for those rejected.\n",
    "ax.scatter(admit['gpa'][admit['admit'] == 0],\n",
    "           admit['admit'][admit['admit'] == 0],\n",
    "           c='orange',\n",
    "           s=100,\n",
    "           alpha=0.7,\n",
    "           label='rejected')\n",
    "\n",
    "# Create scatterplot with blue dots for those rejected.\n",
    "ax.scatter(admit['gpa'][admit['admit'] == 1],\n",
    "           admit['admit'][admit['admit'] == 1],\n",
    "           c='blue',\n",
    "           s=100,\n",
    "           alpha=0.7,\n",
    "           label='admitted')\n",
    "\n",
    "# Set labels of axis.\n",
    "ax.set_ylabel('admitted', fontsize=16)\n",
    "ax.set_xlabel('gpa', fontsize=16)\n",
    "ax.set_title('Predicting Admission from GPA (Prestige = 1)', fontsize=20)\n",
    "\n",
    "ax.set_xlim(1.5, 5)\n",
    "ax.set_ylim(-0.25, 1.25)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Note:** Unless we predict exactly one value, linear regression will always predict values between $-\\infty$ and $+\\infty$. There are real-world cases when this is inappropriate... because we may want to just make predictions on a smaller set or range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pred-binary'></a>\n",
    "\n",
    "## Predicting a Binary Class\n",
    "\n",
    "---\n",
    "\n",
    "In our case we have two classes: `1=admitted` and `0=rejected`.\n",
    "\n",
    "The logistic regression is still solving for $\\hat{Y}$. However, in our binary classification case, $\\hat{Y}$ will be the probability of `y` being one of the classes.\n",
    "\n",
    "### $$\\hat{Y} = P(Y = 1)$$\n",
    "\n",
    "We'll still try to fit a \"line\" of best fit to this... except it won't be perfectly linear. We need to *guarantee* that the right-hand side of the regression equation will evaluate to a probability. (That is, some number between 0 and 1!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Logit Link Function (advanced)\n",
    "\n",
    "---\n",
    "\n",
    "We will use something called a **link function** to effectively \"bend\" our line of best fit so that it is a curve of best fit that matches the range or set of values in which we're interested.\n",
    "\n",
    "For logistic regression, that specific link function that transforms (\"bends\") our line is known as the **logit** link.\n",
    "\n",
    "### $$\\text{logit}\\left(P(Y = 1)\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p$$\n",
    "\n",
    "### $$\\log\\left(\\frac{P(Y = 1)}{1 - P(Y = 1)}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p$$\n",
    "\n",
    "Equivalently, we assume that each independent variable $X_i$ is linearly related to the **log of the odds of success**.\n",
    "\n",
    "Remember, the purpose of the link function is to bend our line of best fit.\n",
    "- This is convenient because we can have any values of $X$ inputs that we want, and we'll only ever predict between 0 and 1!\n",
    "- However, interpreting a one-unit change gets a little harder. (More on this later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logistic-viz'></a>\n",
    "\n",
    "## Fitting and making predictions with the logistic regression model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Instantiate our model.\n",
    "\n",
    "\n",
    "# Step 2: Fit our model.\n",
    "\n",
    "\n",
    "print(f'Logistic Regression Intercept: {logreg.intercept_}')\n",
    "print(f'Logistic Regression Coefficient: {logreg.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two methods in `sklearn` we will use **a lot** for the rest of this course and beyond:\n",
    "- `.predict()`\n",
    "- `.predict_proba()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 (part 1): Generate predicted values.\n",
    "print(f'Logreg predicted values: {}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 (part 2): Generate predicted probabilities.\n",
    "print(f'Logreg predicted probabilities: {}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='log-odds'></a>\n",
    "\n",
    "### Using the log-odds —the natural logarithm of the odds.\n",
    "\n",
    "The combination of converting the \"probability of success\" to \"odds of success,\" then taking the logarithm of that is called the **logit link function**.\n",
    "\n",
    "### $$\\text{logit}\\big(P(Y=1)\\big) = \\log\\bigg(\\frac{P(Y=1)}{1-P(Y=1)}\\bigg) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p$$\n",
    "\n",
    "We've bent our line how we want... but how do we interpret our coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='odds-ratios'></a>\n",
    "### Odds\n",
    "\n",
    "Probabilities and odds represent the same thing in different ways. The odds for probability **p** is defined as:\n",
    "\n",
    "### $$\\text{odds}(p) = \\frac{p}{1-p}$$\n",
    "\n",
    "The odds of a probability is a measure of how many times as likely an event is to happen than it is to not happen.\n",
    "\n",
    "**Example**: Suppose I'm looking at the probability and odds of a specific horse, \"Secretariat,\" winning a race.\n",
    "\n",
    "- When **`p = 0.5`**: **`odds = 1`**\n",
    "    - The horse Secretariat is as likely to win as it is to lose.\n",
    "- When **`p = 0.75`**: **`odds = 3`**\n",
    "    - The horse Secretariat is three times as likely to win as it is to lose.\n",
    "- When **`p = 0.40`**: **`odds = 0.666..`**\n",
    "   - The horse Secretariat is two-thirds as likely to win as it is to lose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called odds to calculate the odds of success.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a range of probabilities.\n",
    "probabilities = np.linspace(0.005, .99, 200)\n",
    "\n",
    "# Generate list of odds.\n",
    "odds_list = [odds(proba) for proba in probabilities]\n",
    "\n",
    "# Create figure.\n",
    "plt.figure(figsize=(11,6))\n",
    "\n",
    "# Plot blue line for odds as probability goes from 0.5% to 99%.\n",
    "plt.plot(probabilities,\n",
    "         odds_list)\n",
    "\n",
    "# Plot red dashed line to visualize odds when probability is 50%.\n",
    "plt.vlines(.5, 0, 100, linestyles=\"dashed\", color='red')\n",
    "plt.text(.33, 50, \"odds(.5) = 1\", fontsize=15, color = 'red')\n",
    "\n",
    "# Plot orange dotted line to visualize odds when probability is 66.67%.\n",
    "plt.vlines(.6667, 0, 100, linestyles=\"dotted\", color='orange')\n",
    "plt.text(.68, 50, \"odds(2/3) = 2\", fontsize=15, color='orange')\n",
    "\n",
    "# Annotate blue line when probability is 100%.\n",
    "plt.text(1, 100, \"odds(1) = $\\infty$\", fontsize=15, color='blue')\n",
    "\n",
    "# Make title.\n",
    "plt.title(\"If the probability of success is 50%, then the odds of success are 1.\\nIf the probability of success is 100%, then the odds of success are $\\infty$.\",\n",
    "          ha = 'left',\n",
    "          position = (0,1),\n",
    "          fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting a one-unit change in $X_i$.\n",
    "\n",
    "$$\\log\\bigg(\\frac{P(Y=1)}{1-P(Y=1)}\\bigg) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p$$\n",
    "\n",
    "Given this model, a one-unit change in $X_i$ implies a $\\beta_i$ unit change in the log odds of success.\n",
    "\n",
    "**This is annoying**.\n",
    "\n",
    "We often convert log-odds back to \"regular odds\" when interpreting our coefficient... our mind understands odds better than the log of odds.\n",
    "\n",
    "So, let's get rid of the log on the left-hand side. Mathematically, we do this by \"exponentiating\" each side.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\log\\bigg(\\frac{P(Y=1)}{1-P(Y=1)}\\bigg) &=& \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p \\\\\n",
    "\\Rightarrow e^{\\Bigg\\{\\log\\bigg(\\frac{P(Y=1)}{1-P(Y=1)}\\bigg)\\Bigg\\}} &=& e^{\\Bigg\\{\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p\\Bigg\\}} \\\\\n",
    "\\Rightarrow \\exp{\\Bigg\\{\\log\\bigg(\\frac{P(Y=1)}{1-P(Y=1)}\\bigg)\\Bigg\\}} &=& \\exp{\\Bigg\\{\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p\\Bigg\\}} \\\\\n",
    "\\Rightarrow \\frac{P(Y=1)}{1-P(Y=1)} &=& \\exp{\\Bigg\\{\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p\\Bigg\\}} \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "**Interpretation**: A one-unit change in $X_i$ means that success is $e^{\\beta_i}$ times as likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> I want to interpret the coefficient $\\hat{\\beta}_1$ for my logistic regression model. How would I interpret this coefficient?</summary>\n",
    "    \n",
    "- Our model is that $ln\\bigg(\\frac{P(admit=1)}{1-P(admit=1)}\\bigg) = \\beta_0 + \\beta_1\\text{GPA}$.\n",
    "- As GPA increases by 1, the log-odds of someone being admitted increases by 2.20.\n",
    "- As GPA increases by 1, someone is $e^{2.20}$ times as likely to be admitted.\n",
    "- As GPA increases by 1, someone is about 9.04 times as likely to be admitted to grad school.\n",
    "</details>\n",
    "\n",
    "> Hint: Use the [np.exp](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Logistic Regression Coefficient: {logreg.coef_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use np.exp() to exponentiate the coefficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond binary classification problems.\n",
    "\n",
    "Today, we looked at a binary classification problem: was somebody admitted, or not? Many of our classification problems will be binary. Even if the problem isn't binary, we can always force it to be binary. (i.e. Did the stock price increase or decrease?)\n",
    "\n",
    "However, some of our classification problems won't be binary. You'll see an example of this in a later lesson!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "The goal of logistic regression is to find the best-fitting model to describe the relationship between a binary outcome and a set of independent variables.\n",
    "\n",
    "Logistic regression generates the coefficients of a formula to predict a logit transformation of the probability that the characteristic of interest is present.\n",
    "\n",
    "**Benefits of logistic regression include:**\n",
    "- It's a classification algorithm that shares similar properties to linear regression.\n",
    "- It's efficient.\n",
    "- It is by far the most common classification algorithm.\n",
    "- The coefficients in a logistic regression model are interpretable (albeit somewhat complex); they represent the change in log-odds caused by the input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='solving-beta'></a>\n",
    "## Solving For the Beta Coefficients (bonus)\n",
    "\n",
    "---\n",
    "\n",
    "Logistic regression minimizes the \"deviance,\" which is similar to the residual sum of squares in linear regression, but is a more general form. \n",
    "\n",
    "**There's no closed-form solution to the beta coefficients like in linear regression, and the betas are found through optimization procedures.**\n",
    "- We can't just do $\\hat{\\beta} = (X^TX)^{-1}X^Ty$ like we can in linear regression!\n",
    "\n",
    "If you're particularly interested in the math, here are two helpful resources:\n",
    "- [A good blog post](http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/) on the logistic regression beta coefficient derivation.\n",
    "- [This paper](https://www.stat.cmu.edu/~cshalizi/402/lectures/14-logistic-regression/lecture-14.pdf) is also a good reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logistic'></a>\n",
    "## The Logistic Function (bonus)\n",
    "\n",
    "---\n",
    "\n",
    "The inverse function of the logit is called the **logistic function**. \n",
    "\n",
    "By inverting the logit, we can have the right side of our regression equation solve explicitly for $P(Y = 1)$:\n",
    "\n",
    "### $$P(Y=1) = logit^{-1}\\left(\\beta_0 + \\sum_{j}^p\\beta_jX_j\\right)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "### $$logit^{-1}(a) = logistic(a) = \\frac{e^{a}}{e^{a} + 1}$$ \n",
    "\n",
    "Giving us:\n",
    "\n",
    "### $$P(Y=1) = \\frac{e^{\\left(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p\\right)}}{e^{\\left(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p\\right)}+1}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
