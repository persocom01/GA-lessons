{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Cross-Validation Lesson\n",
    "\n",
    "_Authors: Matt Brems (DC), Riley Dallas(AUS)_\n",
    "\n",
    "---\n",
    "\n",
    "## Review of Linear Regression\n",
    "---\n",
    "\n",
    "- Linear regression is a way for us to relate some dependent variable $Y$ to independent variables $X_1$,$\\ldots$,$X_p$.\n",
    "- We might write this out in one of the following two forms:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "Y &=& \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p +\\varepsilon\\\\\n",
    "\\mathbf{Y} &=& \\mathbf{X \\beta + \\varepsilon}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "<details><summary>\n",
    "There are four assumptions to the simple linear regression model and five assumptions to the multiple linear regression model.\n",
    "</summary>\n",
    "1. **Linearity:** $Y$ is linearly related to $X_i$ for all $i$.<br>\n",
    "2. **Independence:** Each residual $\\varepsilon_i$ is independent of $\\varepsilon_j$ for all $i\\neq j$.<br>\n",
    "3. **Normality:** The errors (residuals) follow a Normal distribution with mean 0.<br>\n",
    "4. **Equality of Variance:** The errors (residuals) should have a roughly consistent pattern, regardless of the value of $X_i$. (There should be no discernible relationship between $X_i$ and the residuals.)<br>\n",
    "5. **Independence Part II:** $X_i$ is independent of $X_j$ for all $i\\neq j$.\n",
    "</details>\n",
    "\n",
    "We can measure the performance of our model by using mean squared error (MSE).\n",
    "\n",
    "## Feature Engineering\n",
    "---\n",
    "\n",
    "- If I use degrees Fahrenheit to predict how much a substance will expand or inches of rain to predict traffic accidents, people outside the United States may have a tougher time understanding my work.\n",
    "- If I use straight line distance (as the crow flies) between two locations, my estimated time of arrival in a taxi or a Lyft is going to be pretty bad.\n",
    "- If I put text into my model without some sort of preprocessing, my computer isn't going to understand how to handle it.\n",
    "\n",
    "Suffice it to say: If your features (variables) aren't good, your predictions and inferences won't be good!\n",
    "\n",
    "#### What is feature engineering?\n",
    "\n",
    "\"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" - Andrew Ng\n",
    "\n",
    "**Feature engineering** is the term broadly applied to the creation and manipulation of features (variables) used in machine learning algorithms.\n",
    "\n",
    "Unless we're working with the same data over and over again, this isn't something we can automate. It will require creativity and a good, thorough understanding of our data.\n",
    "\n",
    "#### The Process of Data Science\n",
    "1. Data Gathering\n",
    "2. Data Cleaning/Munging\n",
    "3. EDA\n",
    "4. Modeling\n",
    "5. Reporting\n",
    "\n",
    "Feature engineering will straddle all five of these steps, but mostly focus on steps 2 and 3.\n",
    "\n",
    "#### [The Process of Feature Engineering](https://www.youtube.com/watch?v=drUToKxEAUA)\n",
    "1. Brainstorming or testing features.\n",
    "2. Deciding what features to create.\n",
    "3. Creating features.\n",
    "4. Checking how the features work with your model.\n",
    "5. Improving features (if needed).\n",
    "6. Return to step 1.\n",
    "7. \"Do data science!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "---\n",
    "\n",
    "We'll need the following libraries for today's lesson:\n",
    "\n",
    "1. `pandas`\n",
    "2. `numpy`\n",
    "3. `seaborn`\n",
    "4. `train_test_split` and `cross_val_score` from `sklearn`'s `model_selection` module\n",
    "5. a `LinearRegression` mode from `sklearn`'s `linear_model` module\n",
    "6. `StandardScaler` and `PolynomialFeatures` from `sklearn`'s `preprocessing` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "---\n",
    "\n",
    "Today's [dataset](http://www-bcf.usc.edu/~gareth/ISL/data.html) (`Advertising.csv`) is from the [ISLR website](http://www-bcf.usc.edu/~gareth/ISL/). \n",
    "\n",
    "Drop `Unnamed: 0` once you've loaded the csv into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our features matrix (`X`) and target vector (`y`)\n",
    "---\n",
    "\n",
    "The following columns will be our features:\n",
    "- `TV`\n",
    "- `radio`\n",
    "- `newspaper`\n",
    "\n",
    "The `sales` column is our target; the column we're trying to predict.\n",
    "\n",
    "In the cell below, create your `X` and `y` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression model\n",
    "---\n",
    "\n",
    "In the cell below, create a `LinearRegression` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline score\n",
    "---\n",
    "\n",
    "In the cell below, use `cross_val_score` to get a baseline $R^2$ for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction terms\n",
    "---\n",
    "\n",
    "Imagine seeing a commercial on TV, and then hearing a commercial by the same company a few days later. If you eventually purchase the product in question, was it the TV spot that swayed you or the radio commercial...or was a it a synergy of the two commercials that did the trick?\n",
    "\n",
    "Interaction terms allow us to see what affect the **combination** of two features have on our label. For example, I'm not a fan of plain hot dogs, and I never eat mustard by itself. But I do enjoy hot dogs **with** mustard.\n",
    "\n",
    "To create an interaction term, we simply do a pairwise multiplication of two columns:\n",
    "```python\n",
    "df['Interaction Col'] = df['Column 1'] * df['Column 2']\n",
    "```\n",
    "\n",
    "In the cell below, create an interaction column between `TV` and `radio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Create a heatmap\n",
    "---\n",
    "\n",
    "In the cell below, create a heatmap in `seaborn` to show how our `TV * radio` interaction term correlates to all the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Pairplot\n",
    "---\n",
    "\n",
    "Now use `seaborn`'s `.pairplot()` method to create scatterplots with our new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our features matrix (`X`) and target vector (`y`)\n",
    "---\n",
    "\n",
    "Our new column is highly correlated to our label, so let's incorporate it in our features matrix:\n",
    "- `TV`\n",
    "- `radio`\n",
    "- `newspaper`\n",
    "- `TV * radio` interaction term\n",
    "\n",
    "The `sales` column is our label: the column we're trying to predict.\n",
    "\n",
    "In the cell below, create your `X` and `y` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "---\n",
    "\n",
    "Use `cross_val_score` to see how much our new feature moves the $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `PolynomialFeatures`\n",
    "---\n",
    "\n",
    "Congrats! We struck gold on our first interaction term. The question now is how do we create interaction terms **en masse**? In our small dataset, we have three interaction terms:\n",
    "\n",
    "1. `TV * radio`\n",
    "2. `TV * newspaper`\n",
    "3. `radio * newspaper`\n",
    "\n",
    "With larger datasets, manually creating interaction terms would quickly become untenable. That's where `sklearn`'s `PolynomialFeatures` comes in. `PolynomialFeatures` will return 9 columns from our original features matrix:\n",
    "\n",
    "1. The original `TV` column\n",
    "2. The original `radio` column\n",
    "3. The original `newspaper` column\n",
    "4. `TV^2` (`TV` squared)\n",
    "5. `radio^2` (`radio` squared)\n",
    "6. `TV^2` (`TV` squared)\n",
    "7. `TV radio` interaction\n",
    "8. `TV newspaper` interaction\n",
    "9. `radio newspaper` interaction\n",
    "\n",
    "In the cells provided, use `PolynomialFeatures` to transform `X`. **NOTE**: You'll need to set `include_bias=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate features to be the original columns: TV, radio, newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View X_poly in a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "---\n",
    "\n",
    "Use `cross_val_score` to see how much `PolynomialFeatures` affects the $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "---\n",
    "\n",
    "Throughout this course, we'll encounter several models that require you to scale your data prior to modeling. Imagine you were trying predict the price of a house, and you had two features: square footage and number of bedrooms. These two features are on different scales, separated by a few orders of magnitude (square footage is in the thousands and number of bedrooms is in the single digits). \n",
    "\n",
    "When we scale our data, we essentially convert our columns into Z-scores. \n",
    "\n",
    "### ${x- \\mu \\over \\sigma}$\n",
    "\n",
    "In the cell provided, scale the `TV` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `StandardScaler`\n",
    "---\n",
    "\n",
    "Rather than doing this manually for each column, `sklearn` has a `StandardScaler` class that allows us to scale our entire dataset in a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummies\n",
    "---\n",
    "\n",
    "When we dummy a column, we're converting a categorical column into a one-hot encoded matrix. `pandas` allows us to do this with the `pd.get_dummies()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
