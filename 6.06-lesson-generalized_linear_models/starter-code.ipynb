{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Generalized Linear Models\n",
    "\n",
    "_Authors: Tim Book, Justin Pounders (ATL), Matt Brems_\n",
    "\n",
    "### Learning Objectives\n",
    "*After this lesson, students will be able to:*\n",
    "\n",
    "1. Describe generalized linear models.\n",
    "2. Fit Poisson and Gamma regression models in `statsmodels`.\n",
    "3. Interpret coefficients from Poisson and Gamma regression models.\n",
    "4. Describe iteratively reweighted least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why linear models?\n",
    "\n",
    "So far, we've learned about a lot of different models:\n",
    "- linear regression\n",
    "- logistic regression\n",
    "- $k$-nearest neighbors\n",
    "- Naive Bayes models\n",
    "- decision trees\n",
    "- bagged tree models\n",
    "- boosted tree models\n",
    "- support vector machines\n",
    "\n",
    "<details><summary>Why might we choose to fit a linear or logistic regression model instead of a different type of model?</summary>\n",
    "    \n",
    "- Linear and logistic regression models are very interpretable.\n",
    "- We can easily quantify the effect of a one-unit change in the independent variables on the dependent variable.\n",
    "- Because they make so many assumptions, these tend to be simpler models (and thus have lower variance) than other models.\n",
    "</details>\n",
    "\n",
    "### Assumptions of a Linear Regression Model\n",
    "When fitting a linear regression model, there are five assumptions we make:\n",
    "- Linearity (of Y and $X_i$)\n",
    "- Independence (of errors)\n",
    "- Normality (of errors)\n",
    "- Equality of Variance (of errors)\n",
    "- Independence (of our independent variables)\n",
    "\n",
    "## Rewriting the Linear Regression Model\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "Y_i &=& \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_pX_{pi} + \\varepsilon_i \\\\\n",
    "\\varepsilon_i &\\sim& N(0, \\sigma) \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Visually, we get this:\n",
    "![](./images/normal_linear_model.jpg)\n",
    "\n",
    "Rather than write these in two lines, we can combine them into one line:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\Rightarrow Y_i &\\sim& N\\Big(\\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_pX_{pi}, \\sigma\\Big) \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "We can interpret this as:\n",
    "> \"Our observations $Y_i$ follow a Normal distribution with mean $\\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_pX_{pi}$ and standard deviation $\\sigma$.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewriting the Logistic Regression Model\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>The above model was a pretty poor model for binary data. Why?</summary>\n",
    "\n",
    "- We would predict values outside of [0,1].\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Previously, when have we used the Bernoulli distribution?</summary>\n",
    "\n",
    "- We used the Bernoulli distribution to model binary outcomes (success/failure) with a fixed probability of success.\n",
    "- We can use the Bernoulli distribution to model whether or not one coin flips heads, whether or not one person contracts a disease, or whether or not I eat a burrito today.\n",
    "</details>\n",
    "\n",
    "It might make sense that we can rewrite our logistic regression model in the context of the Bernoulli distribution (just like we did with a linear regression model and the Normal distribution)!\n",
    "\n",
    "We can rewrite that as:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "Y_i &\\sim& Bernoulli(\\pi_i) \\\\\n",
    "\\pi_i &=& logit^{-1}(\\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_pX_{pi})\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "\n",
    "Or, in one line:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "Y_i &\\sim& Bernoulli\\Big(logit^{-1}(\\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_pX_{pi})\\Big) \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Note that \"$logit^{-1}$\" is the inverse logit function; you may remember that the logit function was used to bend the \"line of best fit\" to only make predictions between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models (GLMs)\n",
    "\n",
    "**Generalized linear models** describes all models that take the linear regression model and generalizes it to different situations.\n",
    "\n",
    "- It's not always appropriate for our predictions $\\hat{y}$ to range from $(-\\infty, \\infty)$.\n",
    "\n",
    "All GLMs will have three components:\n",
    "- Linear\n",
    "- Link\n",
    "- Random\n",
    "\n",
    "### The Linear Component (aka \"Systematic Component\")\n",
    "The **linear component** will always be the linear formula $\\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p$.\n",
    "- This ensures that every GLM will have some interpretation for a one-unit increase in each independent variable.\n",
    "<details><summary>By the way...</summary>\n",
    "    Many students have confused the term \"linear model\" with \"no exponentiated terms\" - which is wrong. If you include $x_1$ and $x_1^2$ in your model, it's still a linear model, since we can just write $x_2 = x_1^2$. The term <b>linear model</b> comes from the fact that we can always write this systematic component using vectors like $\\mathbf{x}^T\\beta$\n",
    "</details>\n",
    "\n",
    "### The Link Function\n",
    "The **link function** will transform the linear component into the range of interest.\n",
    "- In logistic regression, we use the **logit function** as the link function, which ensures our predictions are between 0 and 1. However it's not the only choice. We can use the **probit function**, which is the inverse standard normal CDF.\n",
    "- In linear regression, the **identity function** is the link function. (i.e. $f(x) = x$)\n",
    "- The link function is what determines how we interpret a one-unit change in each independent variable!\n",
    "\n",
    "The linear and link component generate $\\hat{Y}_i$ for us. \n",
    "\n",
    "![](../images/log_reg.png)\n",
    "\n",
    "### The Random Component\n",
    "The **random component** connects our predictions $\\hat{Y}_i$ to our observed values $Y_i$ by using a statistical distribution to model our errors. It's \"random\" because it controls what the noise of our model looks like.\n",
    "- In linear regression, the linear and link function predict the line of best fit. However, our observed values aren't always on the line; they're Normally distributed around the line.\n",
    "    * For ordinary least squares linear regression, we use the **normal distribution** for our random component.\n",
    "- In logistic regression, the linear and link function predict a probability for us. However, our observed values are 1s and 0s. By saying $Y_i \\sim \\text{Bernoulli}(\\pi_i)$, we note that our $Y_i$ will be 1s and 0s that are drawn from a $\\text{Bernoulli}(\\pi_i)$ distribution with some predicted value between 0 and 1.\n",
    "    * For logistic regression, we use the **Bernoulli distribution** for our random component\n",
    "\n",
    "To summarize:\n",
    "- The linear component will always be $\\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p$.\n",
    "- The link function will bend the linear component to match the desired range of values.\n",
    "- The random component makes the connection from our predictions to our observed values by summarizing the errors.\n",
    "\n",
    "## Linear and logistic regression models aren't the only GLMs out there!\n",
    "\n",
    "We can use all sorts of probability distributions to build out linear models. These linear models still retain the same benefits of linear and logistic regression (i.e. interpretable, can quantify the effect on Y of a one-unit increase in X) but also can model $Y$ variables that aren't just limited to $(-\\infty, +\\infty)$ or $[0, 1]$.\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>What distribution would you use to model the number of people who drive on the Pennsylvania turnpike (toll road) in the weekend before a holiday? </summary>\n",
    "\n",
    "- Since the number of people passing through a toll road is countable, we would want to use a discrete distribution.\n",
    "- Since we can't have a negative number of people passing through a toll road, we would want to use a discrete distribution that can take on only the values of 0, 1, 2, 3, $\\ldots$\n",
    "- Two good distributions for this would be the [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) or the [Negative Binomial distribution](https://en.wikipedia.org/wiki/Negative_binomial_distribution)!\n",
    "- You can see more about these distributions [here](https://seeing-theory.brown.edu/probability-distributions/index.html#section2).\n",
    "- [Wikipedia article for toll roads](https://en.wikipedia.org/wiki/Toll_road).\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>What distribution would you use to model the amount of time until your Lyft shows up?</summary>\n",
    "\n",
    "- Since the amount of time until your Lyft shows up is uncountably infinite, we would want to use a continuous distribution.\n",
    "- Since we can't wait a negative amount of time, we should use a continuous distribution that can only take on the values of $[0, \\infty)$.\n",
    "- The Gamma distribution is going to be \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `statsmodels` API\n",
    "\n",
    "We will use the `statsmodels` API to explore GLMs in Python.  (`sklearn` does not have a robust implementation for GLMs.)  Documentation and examples for `statsmodels` can be found [here](http://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels.genmod.generalized_linear_model.GLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "When we fit a model in `statsmodels`, we need to add the column of 1s so that we can have an intercept. `statsmodels` will not automatically add an intercept for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Regression\n",
    "\n",
    "**When do we use it?** When we want to model something on the $\\{0,1,2,\\ldots\\}$ range... like number of cars on through a toll road, number of objects sold or number of awards earned!\n",
    "\n",
    "<img src=\"./images/poisson_model.png\" alt=\"poisson_model\" width=\"400\"/>\n",
    "\n",
    "#### Data\n",
    "We'll rely on UCLA's IDRE module.  This one can be found [here](https://stats.idre.ucla.edu/r/dae/poisson-regression/).\n",
    "\n",
    "#### Data Description\n",
    "_The number of awards earned by students at one high school. Predictors of the number of awards earned include the type of program in which the student was enrolled (e.g., vocational, general or academic) and the score on their final exam in math._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>num_awards</th>\n",
       "      <th>prog</th>\n",
       "      <th>math</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  num_awards  prog  math\n",
       "0   45           0     3    41\n",
       "1  108           0     1    41\n",
       "2   15           0     3    44\n",
       "3   67           0     3    42\n",
       "4  153           0     3    40"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data.\n",
    "award = pd.read_csv(\"https://stats.idre.ucla.edu/stat/data/poisson_sim.csv\")\n",
    "\n",
    "# Check first five rows.\n",
    "award.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x18833c45708>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a scatterplot of math vs. number of awards.\n",
    "plt.scatter(award['math'], award['num_awards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18832aace48>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot distribution of awards by program.\n",
    "award.boxplot('num_awards', by='prog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What might we infer from this plot?</summary>\n",
    "\n",
    "- `prog` is definitely not linearly related to the number of awards one receives.\n",
    "- `prog` looks like a categorical variable.\n",
    "- I am aware of this. I'm going to suspend that knowledge for the sake of example.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Set up X.\n",
    "award_dummy = pd.get_dummies(columns=['prog'], data=award, drop_first=True)\n",
    "poi_vars = ['prog_2', 'prog_3', 'math']\n",
    "award_dummy.head()\n",
    "X = sm.add_constant(award_dummy[poi_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up y.\n",
    "y = award['num_awards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Calling Family(..) with a link class as argument is deprecated.\n",
      "Use an instance of a link class instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Fit model.\n",
    "fam = sm.families.Poisson(link=sm.families.links.log)\n",
    "glm_poi = sm.GLM(y, X, family=fam).fit()\n",
    "# In statsmodels, y is the first argument.\n",
    "# In statsmodels, X is the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>num_awards</td>    <th>  No. Observations:  </th>  <td>   200</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   196</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>         <td>Poisson</td>     <th>  Df Model:          </th>  <td>     3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>          <td>log</td>       <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -182.75</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 09 Dec 2019</td> <th>  Deviance:          </th> <td>  189.45</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>11:34:10</td>     <th>  Pearson chi2:      </th>  <td>  212.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>5</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>   -5.2471</td> <td>    0.658</td> <td>   -7.969</td> <td> 0.000</td> <td>   -6.538</td> <td>   -3.957</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prog_2</th> <td>    1.0839</td> <td>    0.358</td> <td>    3.025</td> <td> 0.002</td> <td>    0.382</td> <td>    1.786</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prog_3</th> <td>    0.3698</td> <td>    0.441</td> <td>    0.838</td> <td> 0.402</td> <td>   -0.495</td> <td>    1.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>math</th>   <td>    0.0702</td> <td>    0.011</td> <td>    6.619</td> <td> 0.000</td> <td>    0.049</td> <td>    0.091</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:             num_awards   No. Observations:                  200\n",
       "Model:                            GLM   Df Residuals:                      196\n",
       "Model Family:                 Poisson   Df Model:                            3\n",
       "Link Function:                    log   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -182.75\n",
       "Date:                Mon, 09 Dec 2019   Deviance:                       189.45\n",
       "Time:                        11:34:10   Pearson chi2:                     212.\n",
       "No. Iterations:                     5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -5.2471      0.658     -7.969      0.000      -6.538      -3.957\n",
       "prog_2         1.0839      0.358      3.025      0.002       0.382       1.786\n",
       "prog_3         0.3698      0.441      0.838      0.402      -0.495       1.234\n",
       "math           0.0702      0.011      6.619      0.000       0.049       0.091\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate summary of model.\n",
    "glm_poi.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting Poisson Coefficients\n",
    "\n",
    "Because of the log link function, we interpret a one-unit increase in $X_i$ as follows:\n",
    "\n",
    "\"As $X_i$ increases by 1, I expect $Y$ to increase by a factor of $e^{\\beta_1}$.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.072722704342061"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0.0702)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005262758289699642"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-5.2471)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: For a one-unit increase in `math`, I expect to win $e^{0.0702} \\approx 1.07$ times as many awards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<details><summary>How would you interpret `prog_2`?</summary>\n",
    "    If you are in Program 2, I expect to win $e^{1.0839} \\approx 3$ times as many awards as if you were in Program 1.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gamma Regression\n",
    "\n",
    "**When do we use it?** When we want to model something on the $[0,\\infty)$ range... like time until some event occurs!\n",
    "\n",
    "### The Data\n",
    "The data used from this example come from a 1945 study about and is inspired by [Peter Craigmile's use](http://www.craigmile.com/peter/teaching/7430/notes/7_gamma_influence.pdf) of this example.\n",
    "\n",
    "**Data Description:** _“Hurn, et al. (1945) published data on the clotting time of blood, giving clotting time in seconds for normal plasma diluted to nine different percentage concentrations with prothrombin-free plasma; clotting was induced by two lots of thromboplastin.” [McCullagh and Nelder](http://www.utstat.toronto.edu/~brunner/oldclass/2201s11/readings/glmbook.pdf)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in.\n",
    "clot = pd.read_csv(\"../datasets/clotting.csv\", index_col=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a boxplot of clot_time group by lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot plasma_pct against clot_time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X.\n",
    "X = \n",
    "\n",
    "# Set up y.\n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model.\n",
    "# NOTE: For prediction purposes, the inverse link might actually be best (it's the \"canonical link\")\n",
    "# but the log link allows us to interpret our coefficients.\n",
    "\n",
    "# In statsmodels, y is the first argument.\n",
    "# In statsmodels, X is the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting Gamma Coefficients\n",
    "\n",
    "Because of the log link function (again!), we interpret a one-unit increase in $X_i$ as follows:\n",
    "\n",
    "\"As $X_i$ increases by 1, I expect $Y$ to increase by a factor of $e^{\\beta_1}$.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponentiate our coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: For a one-unit increase in `plasma_pct`, I expect the blood will take $e^{-0.0156} \\approx 26\\%$ as much time to clot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iteratively Reweighted Least Squares\n",
    "\n",
    "When fitting an OLS regression model, we can find our best values for $\\beta$ by solving $\\hat{\\pmb\\beta} = (X^TX)^{-1}X^Ty$.\n",
    "\n",
    "GLMs are typically not \"directly solvable.\" There is [no closed-form solution](http://mathworld.wolfram.com/Closed-FormSolution.html) for generalized linear models!\n",
    "- This includes logistic regression!\n",
    "\n",
    "#### How does the algorithm work?\n",
    "An algorithm called \"iteratively reweighted least squares\" [has been shown](http://www.utstat.toronto.edu/~brunner/oldclass/2201s11/readings/glmbook.pdf) is \"easy\" to implement in a computer.\n",
    "- A solution is initially guessed, then iteratively refined until we converge on an answer.\n",
    "- IRLS is a special cause of a **gradient descent algorithm**. We'll learn about gradient descent tomorrow.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\hat{\\pmb\\beta}_{[1]} &=& (X^TW_1X)^{-1}X^TW_1y \\\\\n",
    "\\Rightarrow \\hat{\\pmb\\beta}_{[2]} &=& (X^TW_2X)^{-1}X^TW_2y \\\\\n",
    "\\Rightarrow \\hat{\\pmb\\beta}_{[3]} &=& (X^TW_3X)^{-1}X^TW_3y \\\\\n",
    "\\Rightarrow \\hat{\\pmb\\beta}_{[4]} &=& (X^TW_4X)^{-1}X^TW_4y \\\\\n",
    "\\Rightarrow \\hat{\\pmb\\beta}_{[5]} &=& (X^TW_5X)^{-1}X^TW_5y \\\\\n",
    "&\\vdots& \\\\\n",
    "\\Rightarrow \\hat{\\pmb\\beta}_{[99]} &=& (X^TW_{99}X)^{-1}X^TW_{99}y \\\\\n",
    "\\Rightarrow \\hat{\\pmb\\beta}_{[100]} &=& (X^TW_{100}X)^{-1}X^TW_{100}y \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "- At each step (\"iteration\"), these weights will change. (\"reweighted\")\n",
    "\n",
    "The default maximum number of iterations for GLMs in `statsmodels` is 100. \n",
    "- If `No. Iterations` is less than 100, that means the algorithm probably converged.\n",
    "- If `No. Iterations` is 100, that means the algorithm probably didn't converge and that the $\\mathbf{\\hat{\\beta}}$ are still changing. Therefore, **your output is unreliable - DO NOT USE IT**. It could also give some information on the \"flatness\" of your error function. Even more than 20 iterations is sketchy.\n",
    "\n",
    "There are potential pitfalls to this algorithm (some of which we'll cover later). However, what you should know:\n",
    "- If you get a `ConvergenceWarning` or any indication that your number of iterations is large, that means that your model did not fit properly and that you should not rely on the results!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did our results converge?\n",
    "results.converged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Today, we:\n",
    "* Learned how to generalize two models we already knew (linear and logistic regression) into a borader category of models (GLMs)\n",
    "* Refamiliarized ourselves with the `statsmodels` API\n",
    "* Learned the components of GLMs, and how to customize them with `statsmodels`:\n",
    "    - Systematic/linear component, ie, our choice of x-variables\n",
    "    - Link function - a function we choose to \"bend\" our response to our y-variable\n",
    "    - Random component - The distribution that represents the data-generation process for our y-variable\n",
    "* Two new linear models:\n",
    "    - **Poisson regression** - For when your y-variable is Poisson distributed. Most commonly used for _count data_.\n",
    "        - e.g. Predicting how many children a couple will have based on age and income\n",
    "    - **Gamma regression** - For when your y-variable is Gamma distributed. Most commonly used for _waiting-time data_.\n",
    "        - e.g. Predicting how long your phone's battery will last based on screentime use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/glm-sheet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Picking other GLMs (BONUS)\n",
    "\n",
    "Chosing the right kind of generalized linear model (GLM) from all possibilities really boils down to picking the \"error.\"\n",
    "\n",
    "The \"error\" model is really telling you how you expect observations to be distributed.  It is a probability distribution.\n",
    "\n",
    "> 1. In traditional linear regression, the error term is a normal distribution.  This means that you expect actual observations to be normally distributed around your line.\n",
    "\n",
    "> 2. In logistic regression, the error term is a Bernoulli distribution.  This means that you expect actual observations to be above (1) or below (0) the logit curve with a certain probability.\n",
    "\n",
    "Choosing the distribution function often points to a link function you should use: [here is a table](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function).\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. If $Y$ is a non-negative integer:\n",
    "   - Poisson regression if mean $\\approx$ variance\n",
    "   - Negative Binomial regression if variance $\\gg$ mean (overdisperse)\n",
    "   - For example,\n",
    "     - Units sold\n",
    "     - Customers through the door\n",
    "     - Patients to the ER\n",
    "     - Number of cars racing the red light\n",
    "2. If $Y$ values represent categories\n",
    "   - Multinomial logistic regression (unordered categories)\n",
    "   - Ordinal logstic regression (ordered categories)\n",
    "   - For example,\n",
    "     - Does a population tend to buy groceries at Whole Foods, Publix or Kroger?\n",
    "     - Will millenials vote democrat, republican or independent?\n",
    "     - Predicting the Amazon star rating of books.\n",
    "3. If $Y$ values are continuous, non-negative\n",
    "   - Gamma regression\n",
    "   - For example,\n",
    "     - How long before my Uber/Lyft gets here?\n",
    "     - Home prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Deviance (BONUS)\n",
    "\n",
    "We've spoken briefly about deviance before as a generalization of the sums of squares of error for generalized linear models.\n",
    "\n",
    "Suppose we have two models:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "Y_{full} &=& \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_kX_k + \\cdots + \\beta_pX_p + \\varepsilon \\\\\n",
    "Y_{reduced} &=& \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_kX_k + \\varepsilon\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "We say that $Y_{reduced}$ is nested in $Y_{full}$, because the reduced model could \"fit inside\" the full. (You can learn more about nested linear regression models [here](http://people.reed.edu/~jones/Courses/P24.pdf), although the ideas approximately hold for generalized linear models as well.)\n",
    "\n",
    "When we have one model nested inside the other, there is a statistical test to see if adding new variables are worth it. (Think about it like looking at the difference in mean squared error or $R^2$ by adding a variable, but getting a $p$-value quantifying whether or not it's worth it!)\n",
    "\n",
    "We calculate the **deviance** of the reduced model and subtract the **deviance** of the full model from it. This difference in deviance follows a [chi-squared distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution) with $p-k$ degrees of freedom. (Note that $p-k$ indicates how many variables we took out of our full model to get to the reduced model!)\n",
    "\n",
    "**This comparison only works with nested models! Do not use if your models are not nested!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model differences\n",
    "from scipy.stats import chi2\n",
    "\n",
    "grad = pd.read_csv(\"https://stats.idre.ucla.edu/stat/data/binary.csv\")\n",
    "\n",
    "# First, build our top model\n",
    "indep_vars = ['gre', 'gpa', 'rank']\n",
    "X = sm.add_constant(grad[indep_vars])\n",
    "y = grad.admit\n",
    "\n",
    "glm_logit = sm.GLM(y, \n",
    "                   X,\n",
    "                   sm.families.Binomial(sm.families.links.logit))\n",
    "results_logit = glm_logit.fit()\n",
    "\n",
    "\n",
    "# Next, let's see if we can safely reduce our model\n",
    "reduced_vars = ['gre', 'gpa']\n",
    "X_reduced = sm.add_constant(grad[reduced_vars])\n",
    "\n",
    "results_reduced = sm.GLM(y,\n",
    "                 X_reduced,\n",
    "                 sm.families.Binomial(sm.families.links.logit)).fit()\n",
    "results_reduced.summary()\n",
    "\n",
    "\n",
    "# Calculate the difference in deviance\n",
    "D = results_reduced.deviance - results_logit.deviance\n",
    "print('Difference in Deviance: ', D)\n",
    "\n",
    "# Check to see if this difference is significant\n",
    "pval = 1 - chi2.cdf(D, df = 1)\n",
    "print('p-value of test of difference: ', pval) # What can we conclude here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0:$ reduced model is better\n",
    "\n",
    "$H_A:$ reduced model is not better\n",
    "\n",
    "Because $p < \\alpha$, we reject $H_0$ and conclude that the reduced model is not better."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
